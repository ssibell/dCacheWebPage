---
layout: page
title: upgrade-1.9.12-to-2.2
permalink: /docs/upgrade-1.9.12-to-2.2/
top_graphic: 1
date: 2017-2-22T00:00
---

By Gerd Behrmann 
behrmann@nordu.net

# Table of contents</h2>



## Introduction



<p>dCache 2.2 is the third long term support release (aka <em>golden

release</em>). It is the result of 12 months of development since the

release of dCache 1.9.12, the previous long term support

release. During these 12 months 4 feature releases (1.9.13, 2.0, 2.1,

and 2.2) were made at regular intervals. This document compiles the

most important information from these four feature releases.</p>



<p>dCache 2.2 will be supported with regular bug fix releases at least

until April 2014. dCache 1.9.12 will be supported until April 2013,

however one should expect the release frequency to drop of as only the

most critical bugs and security issues will be fixed. While the

upgrade path from 1.9.12 to 2.2 is easy, it should be expected that no

direct upgrade path from releases prior to 2.2 to a future (fourth)

long term support release is provided.</p>



<p>Many things have changed between 1.9.12 and 2.2 and this document

does not attempt to describe every little change. The focus is on

changes that affect the upgrade process and on new features. Minor

feature changes and bug fixes are often excluded. There is more

information scattered in the release notes of each individual

release.</p>



<p>The last section of this document contains useful reference

material that should be consulted while reading this document. The

reference material also includes a proposed checklist that may be used

while planning an upgrade to 2.2.</p>



<h2>The filesystem hierarchy standard</h2>



<p>The filesystem hierarchy standard (FHS) provides a set of requirements

and guidelines for file and directory placement under UNIX-like

operating systems.</p>



<p>dCache has traditionally been installed in

<tt>/opt/d-cache</tt>. Although this directory is specified by the

FHS, dCache did not follow the recommendations for the installation in

<tt>/opt</tt>.</p>



<p>dCache is now distributed in two different layouts:</p>



<ul>

<li>the classic layout in <tt>/opt</tt>;</li>

<li>a FHS compliant layout.</li>

</ul>



<p>The FHS packages install the bulk of the static files in

<tt>/usr/share/dcache</tt>, with a few files in <tt>/usr/bin</tt>,

<tt>/usr/sbin</tt> and <tt>/usr/share/man</tt>. Configuration files

are stored under <tt>/etc/dcache</tt> and non-static files are stored

under <tt>/var/lib/dcache</tt> with log files in

<tt>/var/log/dcache</tt>.</p>



<p>The FHS packages automatically create a user account (dcache)

during installation and dCache will drop to this account during

startup. An init script and a logrotation configuration is

automatically installed. Admin door ssh keys are auto created during

installation.</p>



<p>Both layouts are distributed via <a

href="http://www.dcache.org/">www.dcache.org</a>, but the FHS packages

should be preferred. It should be expected that future feature

releases will only use the FHS layout.</p>



<p>Migration from the classic layout to the FHS layout is possible and

recommended, however the procedure is manual and not covered by this

document. This process is described in a <a href="http://trac.dcache.org/wiki/optToUsr">separate document</a>.</p>



<p>We recommend and expect that all users will transition to the FHS

packages. Users that wish to continue installing in <tt>/opt</tt> may

do so using the FHS tarball. This tarball uses an internal layout

similar to the other FHS packages, but can be installed in any

directory. One looses the convenience of the package manager, but

gains flexibility in where to install dCache.</p>



<h2>Version number scheme</h2>



<p>Veteran dCache administrators will note the change in version

number scheme. Starting with the release of dCache 2.0.0 we use the

second digit to distinguish feature releases and the third digit to

distinguish bug fix releases.</p>



<h2>Upgrading from 1.9.12</h2>



<p>No direct upgrade path is provided from releases earlier than

1.9.12. Users of earlier releases should upgrade to 1.9.12 first,

verify that everything works, and subsequently upgrade to 2.2.</p>



<p>We strongly recommend upgrading to the latest release of 1.9.12

before upgrading to 2.2. At the time of writing this is 1.9.12-17.</p>



<p><b>Important:</b> <em>Before</em> upgrading to 2.2, the pool

manager's configuration file has to be regenerated by issuing the

<tt>save</tt> command in the pool manager's admin interface. This has

to be done using at least version 1.9.12-9 or 1.9.13-3. Failing to do

so will prevent the pool manager from starting after upgrade and the

configuration file has to rewritten by hand.</p>



<p>Head nodes of 2.2 are compatible with pools 1.9.12-11 and newer,

1.9.13-4 and newer, 2.0.0 and newer, and 2.1.0 and newer, up to and

including 2.2. Pools from any of these releases can be mixed. The

exception to this rule is if NFS 4 is used; in that case all pools and

head nodes have to be upgraded to 2.2 together.</p>



<p>Beginning with the release of 2.3 (primo July 2011) head nodes will

only be compatible with pools belonging to release 2.2 and newer.</p>



<p>Assuming that NFS 4 not is used, a staged upgrade can be performed

by first updating all head nodes (PNFS manager, pool manager, pin

manager, space manager, SRM, all doors, monitoring services, etc)

while leaving pools on the earlier release. Once the head nodes are

online again and confirmed to be working, pools can be upgraded one by

one while dCache is online. Obviously the service will be slightly

degraded as files on a pool that is being upgraded will be momentarily

unavailable. Please note that staged upgrade is not possible if pool

nodes run any other dCache service (such as doors) besides the pool

service.</p>



<p>The alternative to a staged upgrade is to shutdown all dCache

services and upgrade all nodes.</p>



<p>In either case an in-place upgrade is possible and recommended.</p>



<!--p>The first time dCache is started after the upgrade several

databases will be updated: This includes the Berkeley DB Java Edition

files on pools, the pin manager, the space manager, and a couple of

schema tracking tables used by the SRM. The upgrade should be

transparent, although the first start may be slightly slower than

usual. Due to these schema changes, a downgrade from 1.9.12 is not

possible without expert support and should not be attempted.</p-->



<p>Lots of components have been modified to improve consistency,

robustness, latency, and add new features. In several cases this has

affected the semantics of common operations in dCache. We recommend

reading through the following sections, paying attention to issues

like authorization, file ownership, multihoming, obsolete and

forbidden configuration properties, and init scripts.</p>

















<h2>File access authorization</h2>



<!-- 1.9.13 ACL now part of Chimera, no longer supported for PNFS -->

<p>ACLs are now tightly integrated into Chimera. Consequently, ACL

support for PNFS has been removed. We recommend upgrading to Chimera

if ACL support is needed.</p>



<!-- 1.9.13 ACL command line clients integrated into Chimera -->

<p>The ACL command line tools <tt>chimera-get-acl</tt> and

<tt>chimera-set-acl</tt> have been replaced by the <tt>getfacl</tt>

and <tt>setfacl</tt> subcommands of the Chimera command line tool

<tt>chimera-cli</tt>. Eg. rather than <tt>chimera-get-acl</tt> one now

uses <tt>chimera-cli getfacl</tt>. The arguments of the

<tt>setfacl</tt> command have changed to no longer rely on an explicit

index to order the ACEs. The following is an example of using the

<tt>setfacl</tt> command:</p>



<blockquote>

<tt>

<b>$</b> chimera-cli setfacl /pnfs/desy.de/data USER:123:lfx:A GROUP:123:lfx:D

</tt>

</blockquote>



<!-- 2.1 ACL query and update trough NFS 4 -->



<p>The ACLs can now also be queried and updated through NFS

4. Assuming NFS 4 file system is mounted, one can query and update

ACLs like so:</p>



<blockquote>

<tt>

<b>$</b> nfs4_getfacl /pnfs/desy.de/data/generated/acl-test<br/>

<b>$</b> nfs4_setfacl -a A::tigran@desy.afs:arw

</tt>

</blockquote>



<!-- 2.0 ACL admin functionality part of PnfsManager -->



<p>The admin shell commands previously provided by the <tt>acl</tt>

service, that is, <tt>setfacl</tt> and <tt>getfacl</tt>, are now

integrated into the <tt>pnfsmanager</tt> service. The <tt>acl</tt>

service is obsolete and should be removed from layout files.</p>



<!-- 1.9.13 Full path lookup check for Chimera -->



<p>By default, dCache does not implement the correct POSIX semantics

for lookup permissions: Only lookup permissions of the parent

directory are enforced. This was traditionally done to improve

performance with the PNFS backend, but is now only kept to maintain

backwards compatibility. The default behaviour is unchanged, however

setting the new <tt>pnfsmanager</tt> configuration property

<tt>pnfsVerifyAllLookups</tt> to true will enable POSIX semantics. The

property is only supported for Chimera.</p>



<!-- 1.9.13 Permission checks now integrated into Chimera -->



<p>For Chimera, the authorization checks have been optimized to reduce

the number of database queries involved. This reduces latency on name

space operations and improves Chimera throughput.</p>



<h2>Authentication</h2>



<h3>gPlazma 1</h3>

<!-- 2.2 gPlazma 1 is gone - now a gPlazma 2 plugin -->



<p>gPlazma 1 is no longer supported as a standalone service. The

<tt>gplazma.version</tt> property is obsolete. Support for legacy

authentication and mapping schemes is provided through the new

<tt>gplazma1</tt> plugin for gPlazma 2. This plugin uses the legacy

<tt>/etc/dcache/dcachesrm-gplazma.policy</tt> configuration file. The

default gPlazma 2 configuration in <tt>/etc/dcache/gplazma.conf</tt>

loads the <tt>gplazma1</tt> plugin, which means that existing users of

gPlazma 1 should not have to make any modifications when upgrading to

dCache 2.2. We do however recommend that users migrate away from the

<tt>gplazma1</tt> plugin as soon as possible. Henceforth we will no

longer refer to specific versions of gPlazma.</p>



<h3>New plugins</h3>



<h4>gplazma1</h4>



<p>Supports the legacy <tt>/etc/dcache/dcachesrm-gplazma.policy</tt>

configuration file. Should be used like this:</p>



<blockquote>

<pre>

auth requisite gplazma1

map requisite gplazma1

session requisite gplazma1

</pre>

</blockquote>



<p>Although mixing the <tt>gplazma1</tt> plugin with other plugins is

possible, we recommend migrating away from this plugin as soon as

possible.</p>



<h4>kpwd</h4>

<!-- 1.9.13 kpwd plugin supports passwords -->



<p>gPlazma now supports password authentication through the

<tt>kpwd</tt> plugin. The <tt>kpwd</tt> plugin is not new, however the

support for password based authentication is. The <tt>dcache kpwd</tt>

subcommand of the <tt>dcache</tt> script allows kpwd files to be

manipulated.</p>



<p>Should be used like this:</p>



<blockquote>

<pre>

auth sufficient kpwd

map sufficient kpwd

account sufficient kpwd

session sufficient kpwd

</pre>

</blockquote>



<h4>jaas</h4>



<!-- 2.2 jaas plugin -->



<p>The new <tt>jaas</tt> authentication plugin for gPlazma delegates

password authentication to the <em>Java Authentication and

Authorization Services</em> (JAAS). A valid JAAS configuration has to

provided in <tt>/etc/dcache/jgss.conf</tt>. JAAS has traditionally

been used to support Kerberos authentication in dCache, however the

<tt>jaas</tt> plugin is not limited to the Kerberos use

case. Successful authentication results in a user name principal,

which can be further mapped using one of the mapping plugins.</p>



<p>Should be used like this:</p>



<blockquote>

<pre>

auth sufficient jaas gplazma.jaas.name=gplazma

</pre>

</blockquote>



<p>with a <tt>/etc/dcache/jgss.conf</tt> containing something like:</p>



<blockquote>

<pre>

gplazma {

com.sun.security.auth.module.JndiLoginModule required

user.provider.url="nis://NISServerHostName/NISDomain/user"

group.provider.url="nis://NISServerHostName/NISDomain/system/group";

}

</pre>

</blockquote>



<p>This would cause JAAS to use an external directory service to

verify the password (the JndiLoginModule also supports LDAP).</p>



<p>Note that the gPlazma <tt>jaas</tt> module only supports the

<tt>auth</tt> step. It cannot associate the session with UID, GID or

other information provided by JAAS.</p>



<p>There are lots of third party JAAS login modules available,

allowing you to easily use external password validation services with

dCache.</p>



<h4>krb5</h4>



<!-- 2.0 krb5 plugin for gplazma -->



<p>The new <tt>krb5</tt> mapping plugin for gPlazma is to be used in

conjunction with the <tt>nfsv41</tt> service for Kerberos

authentication (see <a href="#nfs-kerberos">Kerberos

authentication</a>). The <tt>nfsv41</tt> service submits

KerberosPrincipals on the form <tt>user@example.org</tt> to

gPlazma. The <tt>krb5</tt> plugin strips the domain suffix, leaving

only the user name in a user name principal. Other plugins (eg

nsswitch, nis, authzdb) can be used to map the user name to UID and

GID.</p>



<p>Use the plugin like this:</p>



<blockquote>

<pre>

map optional krb5

</pre>

</blockquote>



<h4>nsswitch</h4>



<!-- 2.0 nsswitch plugin for gplazma -->



<p>The new <tt>nsswitch</tt> mapping, identity, and session plugin for

gPlazma allows the systems native name service switch to be used for

mapping user name principals to UID and GID.</p>



<h4>nis</h4>



<!-- 2.1 gPlazma NIS plugin added -->



<p>The new <tt>nis</tt> mapping, identity, and session plugin for

gPlazma allows the Network Information System to be used to map user

name principals to UID, GID and home directory.</p>



<h3>FTP door</h3>

<!-- 1.9.13 Plain FTP uses gPlazma -->



<p>The <tt>ftp</tt> service now supports gPlazma for password

authentication. Use the <tt>useGPlazmaAuthorizationModule</tt> and

<tt>useGPlazmaAuthorizationCell</tt> configuration properties to

control whether gPlazma is used or not. Note that by default gPlazma

is used. Existing deployments will either have to update their gPlazma

configuration to support password authentication or explicitly disable

the use of gPlazma for the <tt>ftp</tt> service.</p>



<h3>HTTP Basic Auth</h3>



<!-- 2.0 HTTP basic auth for webdav door -->



<p>The <tt>webdav</tt> service has been updated to support HTTP Basic

authentication. Password verification is done through gPlazma. Please

note that HTTP Basic authentication over an unencrypted channel is

vulnerable to man-in-the-middle attacks. We strongly recommend only

using HTTP Basic authentication over HTTPS.</p>



<h3>gPlazma cell commands</h3>



<!-- 2.2 New gPlazma cell commands -->



<p>Two new commands were added to the gPlazma the admin

interface.</p>



<p><tt>test login</tt> replaces the existing get mapping command. It

shows the result of a login attempt but is more flexible when

describing which principals have been identified for the user.</p>



<p><tt>explain login</tt> uses the same format as <tt>test login</tt>,

but provides detailed information on how the login was processed. The

result of each processed plugin is explained in a tree structure.</p>



<h2>Pool manager</h2>



<p>Pool manager is used by doors to perform pool

selection. Essentially, pool manager routes transfer to pools, controls

staging from tape, and coordinates pool to pool transfers. Some of the

biggest changes between 1.9.12 and 2.2 happened in the pool manager

and how it is used by doors and pin manager.</p>



<h3>Retry logic</h3>



<!-- 1.9.13 Pool manager retry moved to doors -->

<!-- 1.9.13 Reduced latency in pool selection -->



<p>In previous versions the retry logic in case of pool selection

failures was placed inside pool manager. The consequence of that

design decision was that doors and pin manager would never know what

was happening inside pool manager: Was a file being staged or copied,

or was the transfer suspended because the pool with the file was

offline. Another consequence was that pool manager needed logic to

query file meta data from PNFS manager. The query logic replicated

similar logic already present in doors and would add latency to the

pool selection process.</p>



<p>This behaviour was changed such that pool manager never retries

requests internally. Instead, a pool selection failure causes the

request to fail and be sent back to the door or pin manager. It is at

the discretion of the requester to query PNFS manager for updated meta

data and to retry the request. A consequence is that pool selection

latency is reduced and that the retry logic can be tuned for every

type of door. For instance, xrootd doors can rely on clients retrying

requests and the door thus propagates a failure all the way back to

the client. The SRM door on the other hand may return

SRM_FILE_UNAVAILABLE, letting the client know that the pool with the

file is offline. An FTP door will retry the pool selection

internally.</p>



<p>The logic for suspending requests has not changed. A request that

repeatedly fails will eventually get suspended. As before, doors will

wait for a suspended request to be unsuspended.</p>



<h3>Partition manager</h3>



<p>The pool selection process consists of two parts: The first part

uses an admin configured rule engine. This is called the <em>pool

selection unit</em> and controls to and from which pools particular

files may be written, read or staged. Once a set of candidate pools

has been determined, the second step chooses one of those based on

criteria such as free space and load.</p>



<!-- 2.0 New partition manager -->



<p>The second step is now pluggable. This means that the admin may

choose among several selection algorithms, and that third party

developers may write custom plugins to further tweak and tune

dCache.</p>



<p>To support such pluggable selection algorithms, the partition

manager was rewritten. Partition manager allows several sets of

parameters (partitions) to be defined and associated with different

links. This mechanism has now been extended such that the pool

selection logic itself is part of a partition. The partition is

pluggable such that different links may use different pool selection

strategies.</p>



<!-- 2.0 Several PoolManager.conf commands deprecated -->



<p>As part of the rewrite, several partition manager related commands

have changed. In particular <tt>pm create</tt> now takes a plugin type

parameter and the output format of <tt>pm ls</tt> has changed.</p>



<p><b>Important:</b> Before upgrading, PoolManager.conf MUST be

regenerated by using the <tt>save</tt> command using either 1.9.12-9,

1.9.13-3 or newer. dCache will fail to start if PoolManager.conf

contains any of the obsolete commands. If third party scripts are used

to generate PoolManager.conf, then these scripts will likely have to

be updated (see the release notes of dCache 2.0 for details).</p>



<p>As before, a partition named <tt>default</tt> is hardcoded into the

system. It is used by all links that do not explicitly define the

partition to use. The <tt>default</tt> partitions can however be

recreated/overwritten using the <tt>pm create</tt> command. Doing so

allows the partition type to be set.</p>



<p>Each partition has its own set of parameters and different types of

partitions may support different sets of parameters. All partitions

inherit from a global set of parameters. This set is modified using

<tt>pm set -option=value</tt> (ie without specifying a partition

name). <b>Note:</b> For legacy installations in which the default

partition has not been explicitly recreated, the parameters of the

default partition and the set of parameters inherited by other

partitions are identical. This is done to ensure backwards

compatibility with old pool manager configurations. Once <tt>pm create

default</tt> is used this coupling is removed. We recommend that you

create the default partition explicitly and regenerate the pool

manager configuration. Support for legacy configurations will be

removed in a future update.</p>



<p>The following partition types are supported:</p>



<dl>

<dt>classic</dt>

<dd>The pool selection algorithm used in previous versions of dCache.</dd>

<dt>random</dt>

<dd>Selects a pool randomly.</dd>

<dt>lru</dt>

<dd>Selects the pool that has not been used the longest.</dd>

<dt>wass</dt>

<dd>A new selection algorithm that select pools randomly weighted by

available space, while incorporating age and amount of garbage

collectible files and information about load (see <a

href="#wass">Weighted Available Space Selection</a>). For fresh

installations this is the default partition type.</dd>

<dt>wrandom</dt>

<dd>A simplified version of <em>wass</em>. The behaviour corresponds

to <em>wass</em> with breakeven=0, gap=0, cpucostfactor=0,

spacecostfactor=1, p2p=0, alert=0, halt=0, fallback=0, slope=0,

idle=0.</dd>

</dl>



<p>Third party plugins providing additional partition types can be

installed.</p>



<h3 id="wass">Weighted Available Space Selection</h3>



<!-- 2.0 WASS selection -->



<p><em>wass</em> is the new default pool selection algorithm for new

installations. Existing installations will continue to use the

<em>classic</em> algorithm, however we encourage sites to transition

to the <em>wass</em> algorithm.</p>



<p>The internals of the <em>wass</em> algorithm is much more

complicated than the <em>classic</em> algorithm, however tuning it

should be considerably easier, with less request clumping and more

uniform filling of pools.</p>



<blockquote style="border: dotted 1px grey; padding: 0 10px 0 10px;">



<p><b>How to switch to WASS</b></p>



<p>First, inspect the existing partitions and parameters using <tt>pm

ls -l</tt>. Keep the output for reference. The output will also show

the partition type of each partition. To change the type the partition

has to be recreated using the <tt>pm create</tt> command, eg:</p>



<blockquote>

<tt>

<b>(PoolManager) admin ></b> pm create -type=wass default

</tt>

</blockquote>



<p>Note that this will reset the parameters of the partition. If you

have partition specific parameters, like a replication threshold, then

these need to be set again using <tt>pm set</tt>. It either case, it

is a good idea to reset the <tt>cpucostfactor</tt> and

<tt>spacecostfactor</tt> to their default values, eg:</p>



<blockquote>

<tt>

<b>(PoolManager) admin ></b> pm set default -spacecostfactor=1.0 -cpucostfactor=1.0

</tt>

</blockquote>



</blockquote>



<p>The read pool selection is identical to the <em>classic</em>

algorithm: In essence, the set of pools able to serve the file is

computed, and the pool with the lowest performance cost is

selected. Idle cost cuts, fall back cost cuts, etc are processed as

before. It should be noted that space cost factor and cpu cost factor

have no influence on read pool selection.</p>



<p>The crucial difference in <em>wass</em> is the write pool selection

step: Essentially, pools are selected randomly with a weight computed

from the available space of the pool. The weight is however adjusted

to take the current write load and the garbage collectible files into

account. The higher the current write load (number of write movers),

the less likely a pool is selected. The more garbage collectible files

and the older the last access time of those files, the more likely a

pool is selected.</p>



<blockquote style="border: dotted 1px grey; padding: 0 10px 0 10px;">



<p><b>WASS parameters</b></p>



<p>The WASS algorithm can be tuned by using the following

parameters:</p>



<dl>

<dt>breakeven</dt>

<dd>Set per pool. The value must be between 0.0 and 1.0. High values

of breakeven mean that old files are more valuable. Low values mean

that old files quickly become invaluable. Pools with invaluable files

are more likely to be selected.</dd>



<dt>mover cost factor and cpu cost factor</dt>

<dd>The <em>mover cost factor</em> is set per pool, while the <em>cpu

cost factor</em> is set per partition in the pool manager. The product

of these two factors allows the aggressiveness of the write load

feedback signal to be adjusted. A low value means we expect pools to

scale well with load. A value of 0.0 means that load information is

ignored completely; no feedback is used. A negative value would mean

that a busy pool becomes more attractive; hardly a useful

configuration. The intuitive meaning of the product is that for a

value of f, the probability of choosing a pool is halved for every 1/f

concurrent writers.</dd>



<dt>space cost factor</dt>



<dd>Set per partition in pool manager. Intuitively, the larger the

value the more we prefer pools with free space. For a value of 1.0 the

probability of selecting a pool is proportional with available

space. With smaller values the role of available space drops, until at

0.0 available space no longer influences pool selection. For negative

values the algorithm will give higher preference to pools with less

free space, but that is hardly a useful configuration. At values

higher than 1.0 we give additional preference to pools with free

space.</dd>

</dl>





<p>The following table lists the useful range, the default value, and

special values for all four parameters.</p>



<table class="sortable">

<thead>

<tr>

<th>Parameter</th><th>Useful range</th><th>Default</th><th>Special values</th>

</tr>

</thead>

<tbody>

<tr>

<td>mover cost factor</td>

<td>Non-negative</td>

<td>0.5</td>

<td>0.0 means that write load has no influence on pool selection for

this pool. The useful range of the product of mover cost factor and

cpu cost factor is between 0.0 and 1.0.</td>

</tr>

<tr>

<td>cpu cost factor</td>

<td>Non-negative</td>

<td>1.0</td>

<td>0.0 means that write load has no influence on pool selection for

this partition. The useful range of the product of mover cost factor

and cpu cost factor is between 0.0 and 1.0.</td>

</tr>

<tr>

<td>space cost factor</td>

<td>Non-negative</td>

<td>1.0</td>

<td>0.0 means that free or garbage collectable space has no influence

on pool selection. 1.0 means that pools are selected with a

probability proportional to free space.</td>

</tr>

<tr>

<td>breakeven</td>

<td>[0;1]</td>

<td>0.7</td>

<td>0.0 means that garbage collectible files are considered as free

space. 1.0 means that garbage collectible files are considered as used

space.</td>

</tr>

</tbody>

</table>



<p>It is unlikely that large values for any of the above parameters

leads to useful results.</p>

</blockquote>



<p>Except for the <em>mover cost factor</em> all parameters exist for

the <em>classic</em> algorithm too. They serve similar roles and

increasing or decreasing either parameter has similar effects. However

the details for how these parameters are used has changed

significantly and we strongly recommend starting out with the defaults

values. The exact mathematical meaning is unimportant at this

stage. In our experience the default values are pretty good for most

cases. We expect the tuning process to be iterative, with small

changes to the above four parameters being applied, followed by an

observation phase.</p>



<p>Here are some general tips for tuning: If you want pools with more

free space to fill more quickly then increase the <em>space cost

factor</em>. If you want pools with free space to attract fewer

transfers then reduce the <em>space cost factor</em>. If you want

unaccessed files to be garbage collected more aggressively then reduce

the <em>breakeven</em> parameter. If you want unaccessed files to be

kept longer then increase the <em>breakeven</em> value. If you want

the write load to have a higher impact on write pool selection then

increase either the <em>cpu cost factor</em> or the <em>mover cost

factor</em> (depending on whether the effect should be for individual

pools or for all pools). Conversely, reduce either factor to reduce

the effect write load has on pool selection. Remember than these two

factors are multiplied, so setting either to zero means write load is

not taken into account.</p>



<p>One final point to note compared to the <em>classic</em> algorithm

is that read movers have no direct influence on write pool

selection. This is on purpose and prevents that popular files on

particular pools lead to increased write clumping (which in turns

leads to additional read clumping in the future, resulting in a

negative feedback loop). If pool performance is significantly degraded

by read access patterns, then write movers will eventually accumulate

and result in a lower probability for the pool to be selected for

further writes.</p>



<h3>Wildcards in PSU commands</h3>



<p>Several psu commands now accept wildcard (glob patterns). Use

<tt>help psu</tt> to see which commands accept wildcards.</p>



<h3>Staging without location</h3>



<p>In previous releases pool manager would initiate a stage for any

file if a disk copy was not online. It did so even for files for which

no tape location was known. Starting with dCache 2.1, pool manager

will only generate a stage request for files with a known tape

location.</p>



<p>Sites that rely on the previous behaviour to import data stored to

tape without dCache should contact support@dcache.org.</p>



<h3>Pinning</h3>



<!-- 1.9.13 Reduced latency in pin manager -->



<p>Pin manager is used by SRM and DCAP to trigger staging from tape

and to ensure that the file is not garbage collected for a certain

amount of time. It does this by placing a sticky flag (a pin) on the

file on one of the pools.</p>



<p>In previous versions pin manager would unconditionally delegate

pool selection to pool manager. Now, pin manager will handle some

cases without delegating pool selection to pool manager. This is the

case when a file is already online, or when a disk only file is

offline. In other cases, eg when a pool to pool transfer or a stage

from tape is required, pin manager continues to delegate pool

selection to pool manager.</p>



<p>The benefit of running the pool selection algorithm in pin manager

is that it reduces latency for the common cases that don't require any

internal transfers. It also reduces load on pool manager.</p>



<p>Pool selection in pin manager is implemented by periodically

exporting a snapshot of the configuration and pool status information

from pool manager. Changes to the pool manager configuration may take

up to 30 seconds to propagate to pin manager.</p>





<h2>Chimera</h2>



<h3>Update of stored procedures</h3>



<!-- 1.9.13 Chimera stored procedures need to be regenerated -->



<p>The stored PostgreSQL procedures used by Chimera have been

updated. During upgrade, the SQL script to create/update the stored

procedures has to be applied:</p>



<blockquote>

<tt>

<b>$</b> psql -U postgres -f /usr/share/dcache/chimera/sql/pgsql-procedures.sql chimera

</tt>

</blockquote>



<h3>Directory tags</h3>



<!-- 2.2 Chimera supports change of mode, owner and group on tags -->



<p>The mode, owner and group of directory tags can now be changed

(using the regular chmod, chown and chgrp utilities).</p>



<h3>New checksum command</h3>



<!-- 2.1 Chimera CLI checksum command -->



<p>The command <tt>chimera-cli checksum</tt> was added to query the

checksum of a file.</p>



<h2>Pools</h2>



<h3>Continous checksum validation</h3>



<!-- 1.9.13 Continous checksum validation in pools -->



<p>The checksum scanner has been extended with configurable Continous

background checksumming. Any checksum errors are logged and files are

marked as broken and will not be available for download. The new

<tt>-scrub</tt> option of the <tt>csm set policy</tt> command allows

the feature to be enabled. Eg.</p>



<blockquote>

<tt><b>(pool_0) admin ></b> csm set policy -scrub -limit=2 -period=720</tt>

</blockquote>



<p>Consult the help output of that command for information about

setting throughput limits and scan frequency.</p>



<h3>Pool to pool transfers</h3>

<!-- 2.0 HTTP for pool to pool transfers -->



<p>Pool to pool transfers used to use DCAP. Pool to pool transfers now

use HTTP. This change should be transparent and a WebDAV door is not

required. The primary observable change is that the TCP connection

between source and destination pool is now created from the

destination pool to the source pool. In previous versions the

direction was reversed. Since HTTP is classified as a WAN protocol,

the port used by the source pool to listen for the TCP connection will

be allocated from the WAN port range. The pool CLI commands <tt>pp set

port</tt> and <tt>pp set listen</tt> are deprecated and replaced by

the command <tt>pp interface</tt>. The <tt>pp set listen</tt> command

however calls through to the <tt>pp interface</tt> command, meaning

that old pool configurations will work as is. <b>Important:</b> Due to

this change, new pools are only compatible with pools of releases

1.9.12-11, 1.9.13-4 and newer.</p>





<h2>Info provider</h2>



<!-- 2.0 Info provider config partially moved to dcache.conf -->



<p>Parts of the info-provider configuration that used to be in

<tt>/etc/dcache/info-provider.xml</tt> was moved to the

<tt>/etc/dcache/dcache.conf</tt>. Therefore, you will need to edit

<tt>dcache.conf</tt> so that it contains your configuration. See the

defaults file

<tt>/usr/share/dcache/defaults/info-provider.properties</tt> for the

list of affected properties. You may want to recreate

<tt>/etc/dcache/info-provider.xml</tt> to get rid of the excess

configuration.</p>



<p>Note that, if you are using the default value for an info-provider

property then you do not need to configure that property in

<tt>dcache.conf</tt>: the default value will be used

automatically.</p>



<!-- 2.0 GLUE 2 fixes - v2.0.8 of glue-schema RPM required -->



<p>GLUE2 compliance has been improved. Be sure you have at least

v2.0.8 of glue-schema RPM installed on the node running the info

provider.</p>



<h2>Admin</h2>



<!-- 2.1 ssh2 support for admin shell -->



<p>The admin shell has received numerous improvements. Support for

version 2 of the ssh protocol has been added and is available on port

22224. Support for version 1 still exists and is needed for the dCache

GUI. Although allowed by the protocol, we have not been able to

implement support for both protocols on the same TCP port. Support for

version 1 will eventually be removed.</p>



<!-- 2.1 Color output for admin shell -->

<!-- 2.1 Tab completion for admin shell -->



<p>Color highlighting as well as limited tab completion has been

added.</p>



<h2>Billing</h2>



<!-- 2.1 Billing file templates -->



<p>The output format of messages written to billing files is now

configurable. Have a look at

<tt>/usr/share/dcache/defaults/billing.properties</tt> for details

about available formats.</p>



<!-- 2.1 Billing database reimplemented -->



<p>The billing database schema has changed. The schema is

automatically updated during upgrade. Downgrade is not possible once

upgraded.</p>



<!-- 2.1 Billing plots -->



<p>When using the billing database, the <tt>httpd</tt> service is able

to generate plots from the information in the database. Support is

enabled by setting the <tt>billingToDb</tt> property to <tt>yes</tt>

for the <tt>httpd</tt> service. The plots are available under

<tt>http://admin.example.org:2288/billingHistory/</tt>.</p>





<h2>SRM</h2>



<h3>Listing</h3>



<!-- 1.9.13 Reduced latency in SRM list -->



<p>The SRM list operation provides information about file locality,

among other things. In previous versions the SRM door would query pool

manager to compute the file locality for each file being

listed. dCache now computes the file locality internally in the

SRM. The effect is that latency is reduced. The algorithm relies on a

periodic snapshot of the pool manager configuration and pool state

being transferred from pool manager to the SRM door (similar to how it

is now done in pin manager).</p>



<h3>Pinning</h3>



<!-- 1.9.13 srmPinOnlineFiles -->



<p>The new <tt>srmPinOnlineFiles</tt> property controls whether dCache

pins files that have ONLINE access-latency. If set to false then

dCache will refrain from pinning ONLINE files; dCache still ensures

that the file is available on a read pool before returning the

transfer URL to the client, but no guarantee is made that the file

will not be garbage collected before the transfer URL expires.</p>





<blockquote style="border: dotted 1px grey; padding: 0 10px 0 10px; ">



<p><b>Pinning ONLINE files</b></p>



<p>In previous versions of dCache, when SRM clients asks dCache to

prepare a file for download, the SRM door would always ask the

pin manager to pin the file. This was to ensure that the file is

indeed online, that the file's data is available on a pool the

user may read from, and that the data will not be garbage

collected during the transfer URL's lifetime. A correct

implementation of the SRM protocol must provide these three

guarantees so, for the general case, pinning is required even

when access latency is ONLINE.</p>



<p>The disadvantage of always pinning ONLINE files is that it

introduces latency that, in many cases, is unnecessary; for

example, if a file is permanently available on a pool that the

end user can read from then pinning the file is unnecessary.</p>



<p>Some dCache deployments only store files on pools that are

readable: they have no pools dedicated for writing or staging.

Pinning ONLINE files isn't required for such deployments as

dCache already makes the necessary guarantees.</p>



<p>Other sites may know that the risk of a replicated file becoming

garbage-collected during the lifetime of the transfer URL is

small. If it is garbage-collected then opening the file will

still succeed, but will incur a delay. The site-admin may know

that their user community will accept this small risk in exchange

for improved throughput, in which case pinning ONLINE files is

unnecessary.</p>

</blockquote>



<p>A side effect of disabling <tt>srmPinOnlineFiles</tt> is that it

becomes possible to setup a tapeless system without pin manager. The

default access latency in dCache is however NEARLINE, even when no HSM

system is attached. The access latency has to be changed to ONLINE if

dCache is to run without a pin manager (the system wide default is

controlled through the <tt>DefaultAccessLatency</tt> property in PNFS

manager).</p>



<h3>Unavailable files</h3>



<!-- 1.9.13 SRM_FILE_UNAVAILABLE -->



<p>Due to the changes to pin manager, SRM can now report

SRM_FILE_UNAVAILABLE if files are offline, that is, when the pools

holding the file are down and no tape copy is available.</p>



<h3>Overwrite flag</h3>



<!-- 2.0 overwriteEnabled changed for SRM -->



<p>The SRM protocol allows the client to request that existing files

are overwritten upon upload. By default dCache rejects to follow this

option. Instead it always refuses to overwrite a file.</p>



<p>The configuration property <tt>overwriteEnabled</tt> allowed this

behaviour to be changed. When set to true the SRM would respect the

overwrite request of the client. The way this was implemented however

meant that the option had to be set in all doors that the SRM could

redirect to. Thus one was faced with the choice of either not honoring

the clients request or to enable overwrite by default for all other

protocols too.</p>



<p>The handling of the overwrite flag in the SRM has changed. When

<tt>overwriteEnabled</tt> is set to true and the client requests to

overwrite a file, then the SRM will delete the file before redirecting

to another door. This means that an SRM door can now honor the clients

request even when all other doors are configured not to overwrite

existing files.</p>



<h3>Internal delegation of credentials</h3>



<!-- 2.0 SRM delegation changed -->



<p>srmCopy with GridFTP used to use GSI delegation to transfer

credentials from the SRM door to the pool. This was a slow and CPU

intensive process. The SRM door and pool has now been updated to

transfer the credentials through the dCache internal message passing

mechanism.</p>



<p>We assume that the message passing mechanism is secure. Care should

be taken to properly firewall access to the message passing

system. This should be done whether srmCopy is used or not.</p>





<h3>The <tt>file</tt> protocol</h3>



<!-- 2.1 file:// protocol registered in SRM for NFS -->



<p><tt>nfsv41</tt> doors now register with LoginBroker using the

<tt>file://</tt> protocol. This allows SRM to produce TURLs for this

protocol.</p>



<h3>No kpwd support</h3>



<!--2.2 SRM has no direct kpwd support-->



<p>Support for running the <tt>srm</tt> service so that it directly

uses a custom kpwd file (independent of the rest of dCache) has been

removed. All SRM authentication and authorisation activity must now go

through gPlazma. Note that it is possible to configure SRM to use a

different gPlazma configuration from the rest of dCache by 'embedding'

gPlazma (see <tt>useGPlazmaAuthorizationModule</tt> and

<tt>useGPlazmaAuthorizationCell</tt> options). These options, along

with gPlazma's <tt>kpwd</tt> plugin, allow for an equivalent

configuration.</p>



<h3>SSL</h3>



<!-- 2.2 SSL on SRM enabled by default -->



<p>The srm service now listens on two ports. These are, by default,

8443 (as before) and 8445. Port 8443 continues to be for SRM clients

that use GSI-based communication and the new port is for SRM clients

that use SSL. While the SSL port is configurable, it is recommended to

use the default as this an agreed port-number for SSL-based SRM

traffic.</p>



<h2>NFS</h2>



<!-- 2.1 Kerberos authentication for NFS 4-->



<h3 id="nfs-kerberos">Kerberos authentication</h3>



<p>Support for RPCSEC_GSS security was added to the NFS 4 door. To

enable it, the following configuration is required:</p>



<ul>

<li>nodes running the nfs service (as well as pool nodes in case of

nfsv41) MUST have a valid kerberos configuration, including keytab with

<tt>nfs/nfs-door.example.org@EXAMPLE.ORG</tt>.</li>

<li>dcache.conf should have proper kerberos options defined:

<blockquote>

<pre>

nfs.rpcsec_gss=true

kerberos.realm=EXAMPLE.ORG

kerberos.jaas.config=/etc/dcache/gss.conf

kerberos.key-distribution-center-list=your.kdc.server

</pre>

</blockquote>

</li>

<li>gss.conf should have proper JAAS configuration:

<blockquote>

<pre>

com.sun.security.jgss.accept {

com.sun.security.auth.module.Krb5LoginModule required

doNotPrompt=true

useKeyTab=true

keyTab="${/}etc${/}krb5.keytab"

debug=false

storeKey=true

principal="nfs/nfs-door.example.org@EXAMPLE.ORG";

};

</pre>

</blockquote>

</li>

<li>all nfs clients have to have valid kerberos configuration,

including keytab with

<tt>host/nfs-client.example.org@EXAMPLE.ORG</tt>.</li>

<li>KerberosPrincipal to UID and GID mapping based on

gplazma2.<p>The mapping is typically a two step process, with a

mapping from KerberosPrincipal to UserNamePrincipal using either the

<tt>krb5</tt> plugin (which simply strips the domain suffix, ie it

maps <tt>user@example.org</tt> to <tt>user</tt>) or the

<tt>gridmap</tt> plugin with a mapping like</p>

<blockquote>

<pre>

"user@EXAMPLE.ORG" some-user

</pre>

</blockquote>

<p>followed by a mapping from UserNamePrincipal to UID and GID using

the <tt>authzdb</tt> plugin or maybe the new <tt>nss</tt> plugin.</p>

The <tt>kpwd</tt> plugin can also be used to achieve both mappings in

one plugin.</li>

</ul>



<p>The dCache NFS implementation supports the following RPCSEC_GSS

QOPs (quality of protection):</p>

<dl>

<dt>NONE</dt><dd>authentication only</dd>

<dt>INTEGRITY</dt><dd>RPC requests integrity validation</dd>

<dt>PRIVACY</dt><dd>RPC requests encryption</dd>

</dl>



<p>These correspond to krb5, krb5i and krb5p mount options, for

example:</p>

<blockquote>

<tt>

<b>#</b> mount -o krb5i server:/export /local/path

</tt>

</blockquote>



<p>Notice, that all data access with NFS 4.1 uses the same QOP as it

was specified for mount, e.g, if privacy was requested at the mount

time, then all NFS traffic including data coming from pools will be

encrypted.</p>



<!-- 2.1 ACL query and update trough NFS 4 -->



<h3>ACL query and update</h3>



<p>ACLs can now be queried and updated through a mounted NFS 4.1 file

system. No special configuration is required. Eg:</p>



<blockquote>

<tt>

<b>$</b> nfs4_getfacl /pnfs/desy.de/data/generated/acl-test<br/>

<b>$</b> nfs4_setfacl -a A::tigran@desy.afs:arw

</tt>

</blockquote>



<h2>FTP</h2>



<!-- 2.2 FTP supports rename -->



<p>FTP doors now support renaming of files. This is provided through

the <tt>RNFR</tt> and <tt>RNTO</tt> commands defined by RFC 959.</p>



<p>To use this functionality you must use a client that supports

renaming. UberFTP supports renaming.</p>



<h2>Hopping manager</h2>



<!-- 1.9.13 Service definition for hopping manager -->



<p>A service definition for hopping manager was added. The name of the

new service is <tt>hopping</tt>.</p>



<h2><tt>dcache</tt> script</h2>



<!-- 1.9.13 Bash integration -->



<p>Tab completion for the Bash shell was added. The FHS RPM and DEB

packages automatically install the <tt>dcache.bash-completion</tt>

script.</p>



<!-- 2.1 dcache ports command added -->



<p>The subcommand <tt>dcache ports</tt> lists all used TCP and UDP

ports and port ranges of configured services. Use the command like

this:</p>



<blockquote>

<tt><b>$</b> dcache ports</tt>

<pre>

DOMAIN CELL SERVICE PROTO PORT

dCacheDomain - - TCP 11112

dCacheDomain - - TCP 11113

dCacheDomain - - TCP 11111

dCacheDomain - - UDP 11111

dCacheDomain - - UDP 0

dCacheDomain httpd httpd TCP 2288

dCacheDomain info info TCP 22112

dCacheDomain DCap-gsi-dcache-vm gsidcap TCP 22128

dCacheDomain SRM-dcache-vm srm TCP 8443

dCacheDomain Xrootd-dcache-vm xrootd TCP 1094

dCacheDomain WebDAV-dcache-vm webdav TCP 2880

adminDomain - - UDP 0

adminDomain alm admin TCP 22223

namespaceDomain - - UDP 0

namespaceDomain NFSv3-dcache-vm nfsv3 TCP (111)

namespaceDomain NFSv3-dcache-vm nfsv3 TCP 2049

namespaceDomain NFSv3-dcache-vm nfsv3 UDP (111)

namespaceDomain NFSv3-dcache-vm nfsv3 UDP 2049

pool - - UDP 0

pool pool_0 pool TCP 20000-25000

pool pool_0 pool TCP 33115-33145

pool pool_1 pool TCP 20000-25000

pool pool_1 pool TCP 33115-33145

pool pool_2 pool TCP 20000-25000

pool pool_2 pool TCP 33115-33145

gridftp-Domain - - UDP 0

gridftp-Domain GFTP-dcache-vm gridftp TCP 2811

gridftp-Domain GFTP-dcache-vm gridftp TCP 20000-25000

testDomain - - UDP 0

testDomain pool10 pool TCP 20000-25000

testDomain pool10 pool TCP 33115-33145



Ports with '-' under the CELL and SERVICE columns provide inter-domain

communication for dCache. They are established independently of any service

in the layouts file and are configured by the broker.* family of

properties.



Entries where the port number is zero indicates that a random port number

is chosen. The chosen port is guaranteed not to conflict with already open

ports.

</pre>

</blockquote>



<!-- 2.2 dcache pool convert command -->



<p>The <tt>dcache pool convert</tt> command replaces the existing

procedure for converting between pool meta data repository

formats. The subcommand supports conversion from <tt>file</tt> to

<tt>db</tt> and from <tt>db</tt> to <tt>file</tt>. The

<tt>metaDataRepositoryImport</tt> configuration property is no longer

supported. Use the command like this:</p>



<blockquote>

<tt><b>$</b> dcache pool convert pool_0 db</tt>

<pre>

INFO - Copying 000097E9203C0F264F8380C3014BCF405783 (1 of 540)

INFO - Copying 0000F881F830AF3E471C9263D8752D5A4BA2 (2 of 540)

...

INFO - Copying 0000DBFEDCB237C24DFA92E48ED9FCD6782D (539 of 540)

INFO - Copying 00009AAC974918F4452AB7E08DF97DF477B3 (540 of 540)



The pool meta data database of 'pool_0' was converted from type

org.dcache.pool.repository.meta.file.FileMetaDataRepository to type

org.dcache.pool.repository.meta.db.BerkeleyDBMetaDataRepository. Note that

to use the new meta data store, the pool configuration must be updated by

adjusting the metaDataRepository property, eg, in the layout file:





metaDataRepository=org.dcache.pool.repository.meta.db.BerkeleyDBMetaDataRepository

</pre>

</blockquote>



<!-- 2.2 dcache pool yaml command -->



<p>The <tt>dcache pool yaml</tt> subcommand replaces the

<tt>meta2yaml</tt> utility. The command dumps the pool meta data

repository data to YAML format. Both the <tt>db</tt> and <tt>file</tt>

pool backends are supported. Use the command like this:</p>



<blockquote>

<tt><b>$</b> dcache pool yaml pool_0</tt>

<pre>

000097E9203C0F264F8380C3014BCF405783:

state: CACHED

sticky:

system: -1

storageclass: myStore:STRING

cacheclass: null

bitfileid: &lt;Unknown&gt;

locations:

hsm: osm

filesize: 954896

map:

uid: -1

StoreName: myStore

gid: -1

path: /pnfs/dcache-vm/data/test-1314688795-15

SpaceToken: 18090188

retentionpolicy: REPLICA

accesslatency: ONLINE

0000F881F830AF3E471C9263D8752D5A4BA2:

state: CACHED

sticky:

system: -1

storageclass: myStore:STRING

cacheclass: null

bitfileid: &lt;Unknown&gt;

locations:

hsm: osm

filesize: 954896

map:

uid: -1

StoreName: myStore

gid: -1

path: /pnfs/dcache-vm/data/test-1314817040-7

SpaceToken: 18100092

retentionpolicy: REPLICA

accesslatency: ONLINE

....

</pre>

</blockquote>



<p>Parsers for YAML are available for all major scripting

languages. The <tt>dcache pool yaml</tt> command is the preferred

mechanism for direct access (ie without using the pool) to the meta

data of a dCache pool.</p>



<h2>Configuration</h2>



<p>Two minor additions to the configuration language have been

made. Both additions allow us to catch more misconfigurations and

the additions provide added documentation value.</p>



<!-- 2.1 Immutable properties added (not important, but maybe have

a section in the appendix listing all immutable properties) -->



<p>The <em>immutable</em> annotation means that the value of a

property cannot be modified. We use this annotation to mark properties

for internal use.</p>



<!-- 2.2 one-of annotation on config -->



<p>The <em>one-of</em> annotation limits properties to have one of a

limited set of values. We typically use this annotation for boolean

properties or enumerations.</p>







<h2>Reference material</h2>



<h3>Upgrade checklist</h3>



<p>Use this checklist to plan the upgrade to 2.2. The checklist

focuses on upgrading a single node and focuses on a manual upgrade.

Upgrades involving installation scripts like dCacheConfigure, YAIM, or

site specific deployment scripts are not covered by this process.</p>



<ol>

<li>Login to the admin shell and execute:

<blockquote>

<tt>

<b>(local) admin ></b> cd PoolManager<br/>

<b>(PoolManager) admin ></b> save

</tt>

</blockquote>

</li>



<li>Execute <tt>/usr/bin/dcache stop</tt>.</li>



<li>Install the 2.2 package (RPM, DEB, or PKG).</li>



<li>Execute <tt>/usr/bin/dcache check-config</tt>.</li>



<li>For each generated warning and error, update

<tt>/etc/dcache/dcache.conf</tt> and the layout file (check the tables

below).</li>



<li>Repeat the previous two steps until all warnings and errors are gone.</li>



<li>If Chimera is used then recreate the stored procedures in the

database by executing:



<blockquote>

<tt>

<b>$</b> psql -U postgres -f /usr/share/dcache/chimera/sql/pgsql-procedures.sql chimera

</tt>

</blockquote>



You may have to provide a user name or password depending on your

PostgreSQL configuration.</li>



<li>If this node publishes GLUE information then ensure that the

relevant info provider properties are defined in

<tt>/etc/dcache/dcache.conf</tt>. Consult the tables below for a list

of properties that may need to be defined.</li>



<li>Start dCache by executing <tt>/usr/bin/dcache start</tt>.</li>



<li>Carefully monitor the log files for any sign of trouble.</li>



</ol>



<h3>Terminology</h3>



<table class="sortable">

<thead>

<tr>

<th style="width: 7em">Term</th>

<th>Description</th>

</tr>

</thead>

<tbody>

<tr>

<td>cell</td>

<td>A component of dCache. dCache consists of many cells. A cell must

have a name which is unique within the domain hosting the cell.</td>

</tr>

<tr>

<td>domain</td>

<td>A container hosting one or more dCache cells. A domain runs within

its own process. A domain must have a name which is unique throughout

the dCache instance.</td>

</tr>

<tr>

<td>service</td>

<td>An abstraction used in dCache configuration files to describe

atomic units to add to a domain. A service is typically implemented

through one or more cells.</td>

</tr>

<tr>

<td>layout</td>

<td>Definition of domains and services on a given host. The layout is

specified in the layout file. The layout file may contain both domain-

and service- specific configuration values.</td>

</tr>

<tr>

<td>pool</td>

<td>A service providing physical data storage.</td>

</tr>

</tbody>

</table>



<h3>Services</h3>



<p>This section lists all supported services. Those marked with a

* are services that dCache requires to function

correctly.</p>



<table class="sortable">

<caption>Core services</caption>

<thead>

<tr>

<th>Name</th>

<th>Decscription</th>

</tr>

</thead>

<tbody>

<tr>

<td>broadcast<sup>&lowast;</sup></td>

<td>Internal message broadcast service.</td>

</tr>

<tr>

<td>cleaner<sup>&lowast;</sup></td>

<td>Service to remove files from pools and tapes when the name space

entry is deleted.</td>

</tr>

<tr>

<td>cns</td>

<td>Cell naming service used in conjuction with JMS for well known

name lookup.</td>

</tr>

<tr>

<td>dir</td>

<td>Directory listing support for DCAP.</td>

</tr>

<tr>

<td>gplazma</td>

<td>Authorization cell</td>

</tr>

<tr>

<td>hopping</td>

<td>Internal file transfer orchestration.</td>

</tr>

<tr>

<td>loginbroker</td>

<td>Central registry of all doors. Provides data to SRM for load

balancing.</td>

</tr>

<tr>

<td>pinmanager</td>

<td>Pinning and staging support for SRM and DCAP.</td>

</tr>

<tr>

<td>pnfsmanager<sup>&lowast;</sup></td>

<td>Gateway to name space (either PNFS or Chimera).</td>

</tr>

<tr>

<td>pool<sup>&lowast;</sup></td>

<td>Provides physical data storage.</td>

</tr>

<tr>

<td>poolmanager<sup>&lowast;</sup></td>

<td>Central registry of all pools. Routes transfers to pools, triggers

staging from tape, performs hot spot detection.</td>

</tr>

<tr>

<td>replica</td>

<td>Manages file replication for <em>Resilient dCache</em>.</td>

</tr>

<tr>

<td>spacemanager</td>

<td>Space reservation support for SRM.</td>

</tr>

<tr>

<td>srm-loginbroker</td>

<td>Central registry of all SRM doors.</td>

</tr>

</tbody>

</table>



<table class="sortable">

<caption>Admin and monitoring services</caption>

<thead>

<tr>

<th>Name</th>

<th>Decscription</th>

</tr>

</thead>

<tbody>

<tr>

<td>admin</td>

<td>SSH based admin shell.</td>

</tr>

<tr>

<td>billing<sup>&lowast;</sup></td>

<td>Service for logging to billing files or the billing database.</td>

</tr>

<tr>

<td>httpd</td>

<td>Legacy monitoring portal. <em>Depends on: loginbroker,

topo</em>.</td>

</tr>

<tr>

<td>info</td>

<td>Info service that collects information about the dCache

instance. <em>Recommends: httpd</em></td>

</tr>

<tr>

<td>statistics</td>

<td>Collects usage statistics from all pools and generates reports in

HTML.</td>

</tr>

<tr>

<td>topo</td>

<td>Builds a topology map of all domains and cells in the dCache

instance.</td>

</tr>

<tr>

<td>webadmin</td>

<td>Web admin portal. <em>Depends on: info</em></td>

</tr>

</tbody>

</table>



<table class="sortable">

<caption>Doors</caption>

<thead>

<tr>

<th>Name</th>

<th>Decscription</th>

</tr>

</thead>

<tbody>

<tr>

<td>authdcap</td>

<td>Authenticated DCAP door. <em>Depends on: dir. Recommends:

pinmanager</em>.</td>

</tr>

<tr>

<td>dcap</td>

<td>dCap door. <em>Depends on: dir. Recommends:

pinmanager</em>.</td>

</tr>

<tr>

<td>gsidcap</td>

<td>GSI dCap door. <em>Depends on: dir. Recommends:

pinmanager</em>.</td>

</tr>

<tr>

<td>kerberosdcap</td>

<td>Kerberized dCap door. <em>Depends on: dir. Recommends:

pinmanager</em>.</td>

</tr>

<tr>

<td>ftp</td>

<td>Regular FTP door without strong authentication.</td>

</tr>

<tr>

<td>gridftp</td>

<td>GridFTP door.</td>

</tr>

<tr>

<td>kerberosftp</td>

<td>Kerberized FTP door.</td>

</tr>

<tr>

<td>nfsv3</td>

<td>NFS 3 name space export (only works with Chimera).</td>

</tr>

<tr>

<td>nfsv41</td>

<td>NFS 4.1 door (only works with Chimera).</td>

</tr>

<tr>

<td>srm</td>

<td>SRM door. <em>Depends on: pinmanager,

loginbroker, srm-loginbroker. Recommends:

transfermanagers, spacemanager</em>.</td>

</tr>

<tr>

<td>transfermanagers</td>

<td>Server side srmCopy support for SRM.</td>

</tr>

<tr>

<td>webdav</td>

<td>HTTP and WebDAV door.</td>

</tr>

<tr>

<td>xrootd</td>

<td>XROOT door.</td>

</tr>

</tbody>

</table>



<table class="sortable">

<caption>Obsolete services</caption>

<thead>

<tr>

<th>Name</th>

<th>Reason</th>

</tr>

</thead>

<tbody>

<tr>

<td>acl</td>

<td>Integrated into pnfsmanager service.</td>

</tr>

<tr>

<td>dummy-prestager</td>

<td>DCAP uses pinmanager for staging.</td>

</tr>

</tbody>

</table>



<h3 id="gplazma-plugins">gPlazma 2 plugins</h3>



<p>The following gPlazma 2 plugins ship with dCache and can be used in

<tt>gplazma.conf</tt>. Note that several plugins implement more than

one type. Usually such plugins should be added to all phases supported

by the plugin.</p>



<table class="sortable">

<caption>gPlazma 2 plugins</caption>

<thead>

<tr>

<th>Name</th>

<th>Type</th>

<th>Description</th>

</tr>

</thead>

<tbody>

<tr>

<td>gplazma1</td>

<td>auth</td>

<td>Legacy support for dcachesrm-gplazma.policy configuration.</td>

</tr>

<tr>

<td>jaas</td>

<td>auth</td>

<td>Implements password authentication through the Java Authentcation

and Authorization Services (JAAS). A valid JAAS setup for password

verification has to be defined in etc/jgss.conf. Fails if no password

credential is provided or if JAAS denies the login. A username

principal is generated upon success.</td>

</tr>

<tr>

<td>kpwd</td>

<td>auth</td>

<td>Implements password authentication using the kpwd file. Fails if

no password credential is provided, if the username is not defined in

the kpwd file, if the password is invalid, or if the entry has been

disabled in the kpwd file.</td>

</tr>

<tr>

<td>voms</td>

<td>auth</td>

<td>Validates any VOMS attributes in an X.509 certificate and extracts

all valid FQANs. Requires that a vomsdir is configured. Fails if no

valid FQAN is found.</td>

</tr>

<tr>

<td>x509</td>

<td>auth</td>

<td>Extracts the DN from an X.509 certificate. The certificate chain

is not validated (it is assumed that the door already validated the

chain). The plugin fails if no certificate chain is provided.</td>

</tr>

<tr>

<td>xacml</td>

<td>auth</td>

<td></td>

</tr>

<tr>

<td>authzdb</td>

<td>map</td>

<td>Maps user and group name principals to UID and GID principals

according to a storage authzdb file. The file format does not

distinguish between user names and group names and hence each entry in

the file maps to both a UID and one or more GIDs. Therefore the UID

and the primary GID are determined by the mapping for the primary

group name or user name. The name of that mapping is kept as the user

name of the login and may be used for a session plugin or for

authorization in space manager. Remaining GIDs are collected from

other mappings of available group names.</td>

</tr>

<tr>

<td>gplazma1</td>

<td>maph</td>

<td>Legacy support for dcachesrm-gplazma.policy configuration.</td>

</tr>

<tr>

<td>gridmap</td>

<td>map</td>

<td>Maps DN principals to user name principals according to a

grid-mapfile. Fails if no DN was provided or no mapping is found.</td>

</tr>

<tr>

<td>kpwd</td>

<td>map</td>

<td>Maps user names, DNs and Kerberos principals according to the kpwd

file. Only user names verified by the kpwd auth plugin are

mapped. Fails if nothing was mapped or if the kpwd entry has been

disabled. Maps to user name, UID and GID principals.

</td>

</tr>

<tr>

<td>krb5</td>

<td>map</td>

<td>Maps Kerberos principals to username principals by stripping the

domain suffix.</td>

</tr>

<tr>

<td>nis</td>

<td>map</td>

<td>Maps user name principals to UID and GID through lookup in

NIS.</td>

</tr>

<tr>

<td>nsswitch</td>

<td>map</td>

<td>Maps user name to UID and GID according to the system native Name

Service Switch.</td>

</tr>

<tr>

<td>vorolemap</td>

<td>map</td>

<td>Maps FQAN principals to group name principals according to a

grid-vorolemap file. Each FQAN is mapped to the first entry that is a

prefix of the FQAN. The primary FQAN (the first in the certificate) is

mapped to the primary group name. Fails if no FQAN was provided or no

mapping was found.</td>

</tr>

<tr>

<td>argus</td>

<td>account</td>

<td></td>

</tr>

<tr>

<td>kpwd</td>

<td>account</td>

<td>Fails if the kpwd entry used during the map has been

disabled.</td>

</tr>

<tr>

<td>authzdb</td>

<td>session</td>

<td>Associates a user name with root and home directory and read-only

status according to a storage authzdb file.</td>

</tr>

<tr>

<td>gplazma1</td>

<td>session</td>

<td>Legacy support for dcachesrm-gplazma.policy configuration.</td>

</tr>

<tr>

<td>kpwd</td>

<td>session</td>

<td>Adds home and root directories and read-only status to the

session. Only applies to mappings generated by the kpwd map

plugin.</td>

</tr>

<tr>

<td>nis</td>

<td>session</td>

<td>Associates a user name with a home directory through NIS

lookup. The sessions root directory is always set to root and the

session is newer read-only.</td>

</tr>

<tr>

<td>nsswitch</td>

<td>session</td>

<td>Sets the session home directory and root directory to the file

system root, and sets the session's read-only status to false.</td>

</tr>

<tr>

<td>nis</td>

<td>identity</td>

<td>Maps user name principals to UID and group name principals to

GID.</td>

</tr>

<tr>

<td>nsswitch</td>

<td>identity</td>

<td>Maps user name principals to UID and group name principals to

GID.</td>

</tr>

</tbody>

</table>



<p>Please consult

<tt>/opt/d-cache/share/defaults/gplazma.properties</tt> for details

about available configuration properties.</p>



<h3>Changed properties</h3>



<p>Most configuration properties are unchanged. Some have however been

removed or replaced and others have been added. The following tables

provide an overview of the properties that may need to be changed when

upgrading from dCache 1.9.12 to 2.2.</p>



<table class="sortable">

<caption>Deprecated properties</caption>

<thead>

<tr>

<th>Property</th>

<th>Alternative</th>

<th>Description</th>

</tr>

</thead>

<tbody>

<tr>

<td>gplazmaPolicy</td>

<td>gplazma.legacy.config</td>

<td>Location of legacy gPlazma configuration file.</td>

</tr>

</tbody>

</table>













<table class="sortable">

<caption>New properties</caption>

<thead>

<tr>

<th>Property</th>

<th>Default</th>

<th>Description</th>

</tr>

</thead>

<tbody>

<tr>

<td>pnfsVerifyAllLookups</td>

<td>false</td>

<td>Whether to verify lookup permissions for the entire path.</td>

</tr>

<tr>

<td>srmPinOnlineFiles</td>

<td>true</td>

<td>Whether to pin disk files</td>

</tr>

<tr>

<td>nfs.port</td>

<td>2049</td>

<td>TCP port used by NFS doors.</td>

</tr>

<tr>

<td>nfs.v3</td>

<td>false</td>

<td>Whether to enable NFS 3 support in NFS 4.1 door.</td>

</tr>

<tr>

<td>nfs.domain</td>

<td></td>

<td>The local NFSv4 domain name.</td>

</tr>

<tr>

<td>nfs.idmap.cache.size</td>

<td>512</td>

<td>Principal cache size of NFS door.</td>

</tr>

<tr>

<td>nfs.idmap.cache.timeout</td>

<td>30</td>

<td>Principal cache timeout of NFS door.</td>

</tr>

<tr>

<td>nfs.idmap.cache.timeout.unit</td>

<td>SECONDS</td>

<td>Principal cache timeout unit of NFS door.</td>

</tr>

<tr>

<td>nfs.rpcsec_gss</td>

<td>false</td>

<td>Whether to enable RPCSEC_GSS for NFS 4.1 door.</td>

</tr>

<tr>

<td>info-provider.site-unique-id</td>

<td>EXAMPLESITE-ID</td>

<td>Single words or phrases that describe your site.</td>

</tr>

<tr>

<td>info-provider.se-unique-id</td>

<td>dcache-srm.example.org</td>

<td>Your dCache's Unique ID.</td>

</tr>

<tr>

<td>info-provider.se-name</td>

<td></td>

<td>A human understandable name for your SE.</td>

</tr>

<tr>

<td>info-provider.glue-se-status</td>

<td>UNDEFINEDVALUE</td>

<td>Current status of dCache</td>

</tr>

<tr>

<td>info-provider.dcache-quality-level</td>

<td>UNDEFINEDVALUE</td>

<td>Maturity of the service in terms of quality of the software components.</td>

</tr>

<tr>

<td>info-provider.dcache-architecture</td>

<td>UNDEFINEDVALUE</td>

<td>the architecture of the storage</td>

</tr>

<tr>

<td>info-provider.dit</td>

<td>resource</td>

<td></td>

</tr>

<tr>

<td>info.provider.paths.tape-info</td>

<td>/usr/share/dcache/xml/tape-info-empty.xml</td>

<td>Location of tape accounting information.</td>

</tr>

<tr>

<td>collectorTimeout</td>

<td>5000</td>

<td>Webadmin timeout for the data collecting cell.</td>

</tr>

<tr>

<td>transfersCollectorUpdate</td>

<td>60</td>

<td>Webadmin update time for the data collecting cell.</td>

</tr>

<tr>

<td>webdavBasicAuthentication</td>

<td>false</td>

<td>Whether HTTP Basic authentication is enabled.</td>

</tr>

<tr>

<td>admin.colors.enable</td>

<td>true</td>

<td>Whether to enable color output of admin door.</td>

</tr>

<tr>

<td>sshVersion</td>

<td>both</td>

<td>Which version of the SSH protocol to use for the admin door.</td>

</tr>

<tr>

<td>admin.ssh2AdminPort</td>

<td>22224</td>

<td>Port to use for SSH 2.</td>

</tr>

<tr>

<td>admin.authorizedKey2</td>

<td>/etc/dcache/admin/authorized_keys2</td>

<td>Authorized keys for SSH 2.</td>

</tr>

<tr>

<td>admin.dsaHostKeyPrivate</td>

<td>/etc/dcache/admin/ssh_host_dsa_key</td>

<td>Location of SSH 2 private key.</td>

</tr>

<tr>

<td>admin.dsaHostKeyPublic</td>

<td>/etc/dcache/admin/ssh_host_dsa_key.pub</td>

<td>Location of SSH 2 public key.</td>

</tr>

<tr>

<td>broker.messaging.port</td>

<td>11111</td>

<td>TCP port used for cells messaging.</td>

</tr>

<tr>

<td>broker.client.port</td>

<td>0</td>

<td>UDP port for cells messaging client.</td>

</tr>

<tr>

<td>billing.formats.MoverInfoMessage</td>

<td colspan="2">See /usr/share/dcache/defaults/billing.properties</td>

</tr>

<tr>

<td>billing.formats.RmoveFileInfoMessage</td>

<td colspan="2">See /usr/share/dcache/defaults/billing.properties</td>

</tr>

<tr>

<td>billing.formats.DoorRequestInfoMessage</td>

<td colspan="2">See /usr/share/dcache/defaults/billing.properties</td>

</tr>

<tr>

<td>billing.formats.StorageInfoMessage</td>

<td colspan="2">See /usr/share/dcache/defaults/billing.properties</td>

</tr>

<tr>

<td>gplazma.nis.server</td>

<td>nisserv.domain.com</td>

<td>NIS server contacted by gPlazma NIS plugin.</td>

</tr>

<tr>

<td>gplazma.nis.domain</td>

<td>domain.com</td>

<td>NIS domain used by gPlazma NIS plugin.</td>

</tr>

<tr>

<td>xrootd.gsi.hostcert.key</td>

<td>/etc/grid-security/hostkey.pem</td>

<td>Host key used by xrootd GSI plugin.</td>

</tr>

<tr>

<td>xrootd.gsi.hostcert.cert</td>

<td>/etc/grid-security/hostcert.pem</td>

<td>Host certificated used by xrootd GSI plugin.</td>

</tr>

<tr>

<td>xrootd.gsi.hostcert.refresh</td>

<td>43200</td>

<td>Host certificate reload period used by xrootd GSI plugin.</td>

</tr>

<tr>

<td>xrootd.gsi.hostcert.verify</td>

<td>true</td>

<td></td>

</tr>

<tr>

<td>xrootd.gsi.ca.path</td>

<td>/etc/grid-security/certificates</td>

<td>CA certificates used by xrootd GSI plugin.</td>

</tr>

<tr>

<td>xrootd.gsi.ca.refresh</td>

<td>43200</td>

<td>CA certificate reload period used by xrootd GSI plugin.</td>

</tr>

<tr>

<td>webadmin.warunpackdir</td>

<td>/var/tmp</td>

<td>Place to unpack Webadmin WAR file.</td>

</tr>

<tr>

<td>gplazma.jaas.name</td>

<td>gplazma</td>

<td>JAAS application name.</td>

</tr>

<tr>

<td>ftp.read-only</td>

<td>false (true for weak ftp)</td>

<td>Whether FTP door allows users to modify content.</td>

</tr>

<tr>

<td>srm.ssl.port</td>

<td>8445</td>

<td>TCP port for SRM over SSL.</td>

</tr>

<tr>

<td>httpd.static-content.plots</td>

<td>/var/lib/dcache/plots</td>

<td>Where to look for billing plot files.</td>

</tr>

<tr>

<td>httpd.static-content.plots.subdir</td>

<td>/plots</td>

<td>URI path element for billing plot files.</td>

</tr>

</tbody>

</table>



<table class="sortable">

<caption>Obsolete properties</caption>

<thead>

<tr>

<th>Name</th>

<th>Description</th>

</tr>

</thead>

<tbody>

<tr>

<td>aclTable</td>

<td>ACLs are now part of Chimera</td>

</tr>

<tr>

<td>aclConnDriver</td>

<td>ACLs are now part of Chimera</td>

</tr>

<tr>

<td>aclConnUrl</td>

<td>ACLs are now part of Chimera</td>

</tr>

<tr>

<td>aclConnUser</td>

<td>ACLs are now part of Chimera</td>

</tr>

<tr>

<td>aclConnPaswd</td>

<td>ACLs are now part of Chimera</td>

</tr>

<tr>

<td>gplazma.version</td>

<td>gPlazma 2 is the only version of gPlazma included.</td>

</tr>

<tr>

<td>srmGssMode</td>

<td>SRM over SSL now has a dedicated port.</td>

</tr>

<tr>

<td>billingDb</td>

<td>Use billingLogsDir.</td>

</tr>

</tbody>

</table>



<p>Most of the following forbidden properties were marked as obsolete

or deprecated in 1.9.12.</p>



<table>

<caption>Forbidden properties</caption>

<thead>

<tr>

<th>Property</th>

<th>Alternative</th>

</tr>

</thead>

<tbody>

<tr>

<td>namespaceProvider</td>

<td>dcache.namespace</td>

</tr>

<tr>

<td>webdav.templates.list</td>

<td>webdav.templates.html</td>

</tr>

<tr>

<td>metaDataRepositoryImport</td>

<td>Use <tt>dcache pool convert</tt> command.</td>

</tr>

<tr>

<td>SpaceManagerDefaultAccessLatency</td>

<td>DefaultAccessLatencyForSpaceReservation</td>

</tr>

<tr>

<td>keyBase</td>

<td>dcache.paths.ssh-keys</td>

</tr>

<tr>

<td>kerberosScvPrincipal</td>

<td>kerberos.service-principle-name</td>

</tr>

<tr>

<td>gsiftpDefaultStreamsPerClient</td>

<td>Forbidden by GridFTP protocol.</td>

</tr>

<tr>

<td>gPlazmaNumberOfSimutaneousRequests</td>

<td>gPlazmaNumberOfSimultaneousRequests</td>

</tr>

<tr>

<td>srmDbHost</td>

<td>srmDatabaseHost</td>

</tr>

<tr>

<td>srmPnfsManager</td>

<td>pnfsmanager</td>

</tr>

<tr>

<td>srmPoolManager</td>

<td>poolmanager</td>

</tr>

<tr>

<td>srmNumberOfDaysInDatabaseHistory</td>

<td>srmKeepRequestHistoryPeriod</td>

</tr>

<tr>

<td>srmOldRequestRemovalPeriodSeconds</td>

<td>srmExpiredRequestRemovalPeriod</td>

</tr>

<tr>

<td>srmJdbcMonitoringLogEnabled</td>

<td>srmRequestHsitoryDatabaseEnabled</td>

</tr>

<tr>

<td>srmJdbcSaveCompletedRequestsOnly</td>

<td>srmStoreCompletedRequestsOnly</td>

</tr>

<tr>

<td>srmJdbcEnabled</td>

<td>srmDatabaseEnabled</td>

</tr>

<tr>

<td>java</td>

<td>Use JAVA_HOME environment variable.</td>

</tr>

<tr>

<td>java_options</td>

<td>dcache.java.options or dcache.java.options.extra</td>

</tr>

<tr>

<td>user</td>

<td>dcache.user</td>

</tr>

<tr>

<td>pidDir</td>

<td>dcache.pid.dir</td>

</tr>

<tr>

<td>logArea</td>

<td>dcache.log.dir</td>

</tr>

<tr>

<td>logMode</td>

<td>dcache.log.mode</td>

</tr>

<tr>

<td>classpath</td>

<td>dcache.java.classpath</td>

</tr>

<tr>

<td>librarypath</td>

<td>dcache.java.library.path</td>

</tr>

<tr>

<td>kerberosRealm</td>

<td>kerberos.realm</td>

</tr>

<tr>

<td>kerberosKdcList</td>

<td>kerberos.key-distribution-center-list</td>

</tr>

<tr>

<td>authLoginConfig</td>

<td>kerberos.jaas.config</td>

</tr>

<tr>

<td>messageBroker</td>

<td>broker.scheme</td>

</tr>

<tr>

<td>serviceLocatorHost</td>

<td>broker.host</td>

</tr>

<tr>

<td>serviceLocatorPort</td>

<td>broker.port</td>

</tr>

<tr>

<td>amqHost</td>

<td>broker.amq.host</td>

</tr>

<tr>

<td>amdPort</td>

<td>broker.amq.port</td>

</tr>

<tr>

<td>amqSSLPort</td>

<td>broker.amq.ssl.port</td>

</tr>

<tr>

<td>amqUrl</td>

<td>broker.amq.url</td>

</tr>

<tr>

<td>ourHomeDir</td>

<td>dcache.home</td>

</tr>

<tr>

<td>portBase</td>

<td>set protocol-specific default ports</td>

</tr>

<tr>

<td>httpPortNumber</td>

<td>webdavPort</td>

</tr>

<tr>

<td>httpRootPath</td>

<td>webdavRootPath</td>

</tr>

<tr>

<td>httpAllowedPaths</td>

<td>webdavAllowedPaths</td>

</tr>

<tr>

<td>webdavContextPath</td>

<td>webdav.static-content.location</td>

</tr>

<tr>

<td>cleanerArchive</td>

<td>cleaner.archive</td>

</tr>

<tr>

<td>cleanerDB</td>

<td>cleaner.book-keeping.dir</td>

</tr>

<tr>

<td>cleanerPoolTimeout</td>

<td>cleaner.pool-reply-timeout</td>

</tr>

<tr>

<td>cleanerProcessFilesPerRun</td>

<td>cleaner.max-files-in-message</td>

</tr>

<tr>

<td>cleanerRecover</td>

<td>cleaner.pool-retry</td>

</tr>

<tr>

<td>cleanerRefresh</td>

<td>cleaner.period</td>

</tr>

<tr>

<td>hsmCleaner</td>

<td>cleaner.hsm</td>

</tr>

<tr>

<td>hsmCleanerFlush </td>

<td>cleaner.hsm.flush.period</td>

</tr>

<tr>

<td>hsmCleanerRecover</td>

<td>cleaner.pool-retry</td>

</tr>

<tr>

<td>hsmCleanerRepository</td>

<td>cleaner.hsm.repository.dir</td>

</tr>

<tr>

<td>hsmCleanerRequest</td>

<td>cleaner.hsm.max-files-in-message</td>

</tr>

<tr>

<td>hsmCleanerScan</td>

<td>cleaner.period</td>

</tr>

<tr>

<td>hsmCleanerTimeout</td>

<td>cleaner.hsm.pool-reply-timeout</td>

</tr>

<tr>

<td>hsmCleanerTrash</td>

<td>cleaner.hsm.trash.dir</td>

</tr>

<tr>

<td>hsmCleanerQueue</td>

<td>cleaner.hsm.max-concurrent-requests</td>

</tr>

<tr>

<td>trash</td>

<td>cleaner.trash.dir</td>

</tr>

<tr>

<td>httpHost</td>

<td>info-provider.http.host</td>

</tr>

<tr>

<td>xsltProcessor</td>

<td>info-provider.processor</td>

</tr>

<tr>

<td>xylophoneConfigurationFile </td>

<td>info-provider.configuration.file</td>

</tr>

<tr>

<td>saxonDir </td>

<td>info-provider.saxon.dir</td>

</tr>

<tr>

<td>xylophoneXSLTDir </td>

<td>info-provider.xylophone.dir</td>

</tr>

<tr>

<td>xylophoneConfigurationDir</td>

<td>info-provider.configuration.dir</td>

</tr>

<tr>

<td>images</td>

<td>httpd.static-content.images</td>

</tr>

<tr>

<td>styles</td>

<td>httpd.static-content.styles</td>

</tr>

</tbody>

</table>
